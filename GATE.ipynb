{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATES(nn.Module):\n",
    "    def __init__(self,hidden_dims,A,X,S,R,lambda_=0.01,num_heads=[6,4,2,1]):\n",
    "        super().__init__()\n",
    "        self.lambda_ = lambda_\n",
    "        self.n_layers = len(hidden_dims)-1\n",
    "        self.skip_proj = nn.ModuleList([nn.Linear(hidden_dims[i],hidden_dims[i+1])for i in range(self.n_layers)])\n",
    "        self.W,self.v = self.define_weights(hidden_dims,num_heads)\n",
    "        self.C={}\n",
    "        self.num_heads = num_heads\n",
    "        self.threshold = 0.01\n",
    "        self.A = A\n",
    "        self.X = X\n",
    "        self.S = S\n",
    "        self.R = R\n",
    "        self.apply(self._init_weights)\n",
    "    #前向传播\n",
    "    def forward(self,A,X,R,S):\n",
    "\n",
    "        #编码\n",
    "        H = X\n",
    "        entropy_loss = 0.0 # 注意力熵正则化项\n",
    "        for layer in range(self.n_layers):\n",
    "            H,entropy_loss = self.encoder(A, H, layer)\n",
    "        #最终节点表征\n",
    "        self.H = H\n",
    "\n",
    "        #解码\n",
    "        for layer in range(self.n_layers-1,-1,-1):\n",
    "            H = self.decoder(H,layer)\n",
    "        X_ = H\n",
    "\n",
    "        #节点得重构损失\n",
    "        features_loss = torch.sqrt(torch.sum((X-X_)**2))\n",
    "\n",
    "        #图结构得重构损失\n",
    "        S_emb = self.H[S]\n",
    "        R_emb = self.H[R]\n",
    "        structure_loss = -torch.log(torch.sigmoid(torch.sum(S_emb*R_emb,dim=-1))).sum()\n",
    "\n",
    "        #总损失\n",
    "        self.loss = features_loss + self.lambda_*structure_loss + entropy_loss*0.1\n",
    "\n",
    "        return self.loss,self.H,self.C\n",
    "    #编码器\n",
    "    def encoder(self,A,H,layer):\n",
    "        X = H\n",
    "        H = nn.LeakyReLU(0.2)(H)\n",
    "        H = torch.matmul(H,self.W[layer].weight.t())\n",
    "\n",
    "        #多头注意力\n",
    "        C,entropy = self.graph_attention_layer(A,H,self.v[layer])\n",
    "        self.C[layer] = C\n",
    "\n",
    "        #残差连接\n",
    "        if layer == 0:\n",
    "            self.skip = H\n",
    "        else:\n",
    "            #当维度变化时投影\n",
    "            if H.shape[1] != self.skip.shape[1]:\n",
    "                projected_skip = self.skip_proj[layer](self.skip)\n",
    "                H = H+projected_skip\n",
    "            else:\n",
    "                H = H + self.skip\n",
    "        return torch.matmul(C,X.T),entropy\n",
    "    #解码器\n",
    "    def decoder(self,H,layer):\n",
    "        H = torch.matmul(H,self.W[layer].weight)\n",
    "        H = nn.BatchNorm1d(H.size(1))(H)  # 添加 BatchNorm\n",
    "        H = nn.ReLU()(H)\n",
    "        return torch.matmul(self.C[layer],H)\n",
    "    \n",
    "    def define_weights(self,hidden_dims,num_heads):\n",
    "        #权重定义（支持不同层不同头数）\n",
    "        W = nn.ModuleList()\n",
    "        V = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            in_dim = hidden_dims[i]*num_heads[i] if i > 0 else hidden_dims[i]\n",
    "            out_dim = hidden_dims[i+1]\n",
    "\n",
    "            #投影矩阵\n",
    "            W.append(nn.Linear(in_dim,out_dim))\n",
    "\n",
    "            #每层注意力参数（每头）\n",
    "            head_params = nn.ModuleList([nn.Linear(hidden_dims[i],hidden_dims[i+1],bias=False)for _ in range(num_heads[i])])\n",
    "            V.append(head_params)\n",
    "        return W,V\n",
    "    def graph_attention_layer(self,A,M,v_layer):\n",
    "        '''\n",
    "        全连接图注意力层（带阈值剪枝）\n",
    "        参数：\n",
    "        M:节点特征矩阵[N,d]\n",
    "        v:注意力参数列表，包含两个可学习向量[v1,v2]每个形状[d,1]\n",
    "        layer:层编号\n",
    "        threshold:注意力权重剪枝阈值(默认0.1)\n",
    "        '''\n",
    "        num_heads = len(v_layer)\n",
    "        head_outputs = []\n",
    "        total_entroy = 0.0\n",
    "\n",
    "        for head in range(num_heads):\n",
    "            # 单头注意力计算\n",
    "            f = torch.matmul(M,v_layer[head].weight)\n",
    "            logits = f\n",
    "\n",
    "            logits = logits/(M.size(1)**0.5)\n",
    "            attentions = F.softmax(logits,dim=1)\n",
    "            entropy = -torch.sum(attentions*attentions*torch.log(attentions+1e-10))/(attentions.size(0)*attentions.size(1))\n",
    "            total_entroy += entropy\n",
    "\n",
    "            head_outputs.append(attentions)\n",
    "        \n",
    "        #合并多头结果\n",
    "        combined_attentions = torch.stack(head_outputs,dim=0).mean(dim=0)\n",
    "        return combined_attentions,total_entroy/num_heads\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.ModuleList):  # 针对注意力参数的特殊初始化\n",
    "            for sub_module in m:\n",
    "                if isinstance(sub_module, nn.Linear):\n",
    "                    nn.init.normal_(sub_module.weight, mean=0, std=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#数据归一化\n",
    "df = pd.read_excel('standardized_file.xlsx')\n",
    "for column in df.columns:\n",
    "    df[column] = (df[column]-df[column].mean())/df[column].std()\n",
    "df = df.drop(0,axis=1)\n",
    "data_3d = df.values.reshape(296,22,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cities = 292\n",
    "num_nodes = 22\n",
    "num_features = 5\n",
    "hidden_dims = [5,16,32,16,5]\n",
    "lambda_ = 0.3\n",
    "A = torch.ones(num_nodes,num_nodes,device='cpu')\n",
    "S,R = np.where(A==1)\n",
    "embeddings = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0:Loss=73.3026\n",
      "Epoch20:Loss=58.3728\n",
      "Epoch40:Loss=57.7111\n",
      "Epoch60:Loss=58.7125\n",
      "Epoch80:Loss=57.1865\n",
      "Epoch100:Loss=57.1565\n",
      "Epoch120:Loss=56.8670\n",
      "Epoch140:Loss=56.8652\n",
      "Epoch160:Loss=58.8043\n",
      "Epoch180:Loss=56.7726\n",
      "Epoch0:Loss=39.4928\n",
      "Epoch20:Loss=29.8621\n",
      "Epoch40:Loss=25.2030\n",
      "Epoch60:Loss=29.9943\n",
      "Epoch80:Loss=28.3375\n",
      "Epoch100:Loss=28.4318\n",
      "Epoch120:Loss=26.6754\n",
      "Epoch140:Loss=31.9385\n",
      "Epoch160:Loss=22.8927\n",
      "Epoch180:Loss=21.1126\n",
      "Epoch0:Loss=28.5567\n",
      "Epoch20:Loss=27.1948\n",
      "Epoch40:Loss=19.9365\n",
      "Epoch60:Loss=20.1987\n",
      "Epoch80:Loss=12.2330\n",
      "Epoch100:Loss=24.3314\n",
      "Epoch120:Loss=14.3721\n",
      "Epoch140:Loss=14.1623\n",
      "Epoch160:Loss=11.9638\n",
      "Epoch180:Loss=11.8913\n",
      "Epoch0:Loss=35.7577\n",
      "Epoch20:Loss=35.1664\n",
      "Epoch40:Loss=31.1825\n",
      "Epoch60:Loss=20.8563\n",
      "Epoch80:Loss=19.5978\n",
      "Epoch100:Loss=30.5878\n",
      "Epoch120:Loss=20.8052\n",
      "Epoch140:Loss=19.1375\n",
      "Epoch160:Loss=19.1144\n",
      "Epoch180:Loss=19.7122\n",
      "Epoch0:Loss=20.1925\n",
      "Epoch20:Loss=13.4991\n",
      "Epoch40:Loss=8.4519\n",
      "Epoch60:Loss=8.5862\n",
      "Epoch80:Loss=7.1044\n",
      "Epoch100:Loss=4.7539\n",
      "Epoch120:Loss=4.9791\n",
      "Epoch140:Loss=5.0193\n",
      "Epoch160:Loss=6.2170\n",
      "Epoch180:Loss=4.0702\n",
      "Epoch0:Loss=24.2567\n",
      "Epoch20:Loss=12.0909\n",
      "Epoch40:Loss=9.8447\n",
      "Epoch60:Loss=9.3801\n",
      "Epoch80:Loss=9.7580\n",
      "Epoch100:Loss=8.6444\n",
      "Epoch120:Loss=8.0311\n",
      "Epoch140:Loss=8.3169\n",
      "Epoch160:Loss=9.2816\n",
      "Epoch180:Loss=8.3320\n",
      "Epoch0:Loss=22.0632\n",
      "Epoch20:Loss=7.0643\n",
      "Epoch40:Loss=7.9327\n",
      "Epoch60:Loss=5.5642\n",
      "Epoch80:Loss=6.1756\n",
      "Epoch100:Loss=6.6242\n",
      "Epoch120:Loss=7.3881\n",
      "Epoch140:Loss=5.8463\n",
      "Epoch160:Loss=5.4422\n",
      "Epoch180:Loss=6.0677\n",
      "Epoch0:Loss=26.7224\n",
      "Epoch20:Loss=22.1587\n",
      "Epoch40:Loss=16.2307\n",
      "Epoch60:Loss=14.1020\n",
      "Epoch80:Loss=13.6326\n",
      "Epoch100:Loss=13.3311\n",
      "Epoch120:Loss=13.8369\n",
      "Epoch140:Loss=14.6928\n",
      "Epoch160:Loss=11.3128\n",
      "Epoch180:Loss=12.3235\n",
      "Epoch0:Loss=20.5080\n",
      "Epoch20:Loss=20.4800\n",
      "Epoch40:Loss=20.4771\n",
      "Epoch60:Loss=8.6449\n",
      "Epoch80:Loss=8.5536\n",
      "Epoch100:Loss=8.6999\n",
      "Epoch120:Loss=4.6290\n",
      "Epoch140:Loss=7.2145\n",
      "Epoch160:Loss=4.7952\n",
      "Epoch180:Loss=5.5913\n",
      "Epoch0:Loss=20.4811\n",
      "Epoch20:Loss=18.1935\n",
      "Epoch40:Loss=6.6861\n",
      "Epoch60:Loss=6.3546\n",
      "Epoch80:Loss=5.6409\n",
      "Epoch100:Loss=4.9969\n",
      "Epoch120:Loss=5.5438\n",
      "Epoch140:Loss=4.4549\n",
      "Epoch160:Loss=5.7183\n",
      "Epoch180:Loss=6.6262\n",
      "Epoch0:Loss=21.6686\n",
      "Epoch20:Loss=19.8053\n",
      "Epoch40:Loss=11.9948\n",
      "Epoch60:Loss=5.4321\n",
      "Epoch80:Loss=6.5138\n",
      "Epoch100:Loss=5.5484\n",
      "Epoch120:Loss=5.2354\n",
      "Epoch140:Loss=9.5966\n",
      "Epoch160:Loss=5.4098\n",
      "Epoch180:Loss=4.8555\n",
      "Epoch0:Loss=21.8252\n",
      "Epoch20:Loss=19.5627\n",
      "Epoch40:Loss=19.0355\n",
      "Epoch60:Loss=16.1841\n",
      "Epoch80:Loss=7.0676\n",
      "Epoch100:Loss=9.1252\n",
      "Epoch120:Loss=7.9017\n",
      "Epoch140:Loss=6.6462\n",
      "Epoch160:Loss=5.2238\n",
      "Epoch180:Loss=5.7712\n",
      "Epoch0:Loss=20.9048\n",
      "Epoch20:Loss=19.2448\n",
      "Epoch40:Loss=16.4811\n",
      "Epoch60:Loss=11.1422\n",
      "Epoch80:Loss=11.3104\n",
      "Epoch100:Loss=14.6787\n",
      "Epoch120:Loss=9.0534\n",
      "Epoch140:Loss=6.6964\n",
      "Epoch160:Loss=5.7315\n",
      "Epoch180:Loss=5.0618\n",
      "Epoch0:Loss=25.4523\n",
      "Epoch20:Loss=19.6914\n",
      "Epoch40:Loss=11.4702\n",
      "Epoch60:Loss=13.2770\n",
      "Epoch80:Loss=10.8074\n",
      "Epoch100:Loss=15.0738\n",
      "Epoch120:Loss=9.4840\n",
      "Epoch140:Loss=9.8755\n",
      "Epoch160:Loss=9.1849\n",
      "Epoch180:Loss=8.9357\n",
      "Epoch0:Loss=21.1709\n",
      "Epoch20:Loss=9.3471\n",
      "Epoch40:Loss=6.9547\n",
      "Epoch60:Loss=17.6632\n",
      "Epoch80:Loss=11.1707\n",
      "Epoch100:Loss=7.1450\n",
      "Epoch120:Loss=4.5899\n",
      "Epoch140:Loss=4.6239\n",
      "Epoch160:Loss=4.5566\n",
      "Epoch180:Loss=4.7671\n",
      "Epoch0:Loss=24.7360\n",
      "Epoch20:Loss=10.2371\n",
      "Epoch40:Loss=9.3237\n",
      "Epoch60:Loss=9.7138\n",
      "Epoch80:Loss=9.5362\n",
      "Epoch100:Loss=8.5827\n",
      "Epoch120:Loss=9.0081\n",
      "Epoch140:Loss=8.1337\n",
      "Epoch160:Loss=8.1580\n",
      "Epoch180:Loss=8.3106\n",
      "Epoch0:Loss=23.7794\n",
      "Epoch20:Loss=17.3992\n",
      "Epoch40:Loss=13.0674\n",
      "Epoch60:Loss=9.6113\n",
      "Epoch80:Loss=7.7410\n",
      "Epoch100:Loss=7.7997\n",
      "Epoch120:Loss=8.0681\n",
      "Epoch140:Loss=7.4777\n",
      "Epoch160:Loss=7.1843\n",
      "Epoch180:Loss=7.2759\n",
      "Epoch0:Loss=22.3750\n",
      "Epoch20:Loss=11.7837\n",
      "Epoch40:Loss=16.4138\n",
      "Epoch60:Loss=6.9448\n",
      "Epoch80:Loss=7.0414\n",
      "Epoch100:Loss=8.2476\n",
      "Epoch120:Loss=6.6456\n",
      "Epoch140:Loss=6.2777\n",
      "Epoch160:Loss=6.6431\n",
      "Epoch180:Loss=5.9624\n",
      "Epoch0:Loss=23.6915\n",
      "Epoch20:Loss=13.7701\n",
      "Epoch40:Loss=10.5143\n",
      "Epoch60:Loss=11.3505\n",
      "Epoch80:Loss=10.8016\n",
      "Epoch100:Loss=11.8232\n",
      "Epoch120:Loss=8.8476\n",
      "Epoch140:Loss=8.8356\n",
      "Epoch160:Loss=7.6321\n",
      "Epoch180:Loss=7.8538\n",
      "Epoch0:Loss=21.7232\n",
      "Epoch20:Loss=7.0367\n",
      "Epoch40:Loss=6.2623\n",
      "Epoch60:Loss=6.7249\n",
      "Epoch80:Loss=5.5943\n",
      "Epoch100:Loss=5.1767\n",
      "Epoch120:Loss=5.5102\n",
      "Epoch140:Loss=5.5503\n",
      "Epoch160:Loss=6.4629\n",
      "Epoch180:Loss=5.1172\n",
      "Epoch0:Loss=22.0711\n",
      "Epoch20:Loss=21.1593\n",
      "Epoch40:Loss=10.4220\n",
      "Epoch60:Loss=11.4552\n",
      "Epoch80:Loss=9.5813\n",
      "Epoch100:Loss=8.6117\n",
      "Epoch120:Loss=9.2977\n",
      "Epoch140:Loss=6.4378\n",
      "Epoch160:Loss=6.7700\n",
      "Epoch180:Loss=6.6577\n",
      "Epoch0:Loss=21.7022\n",
      "Epoch20:Loss=19.8982\n",
      "Epoch40:Loss=13.2072\n",
      "Epoch60:Loss=11.5130\n",
      "Epoch80:Loss=9.7736\n",
      "Epoch100:Loss=6.8382\n",
      "Epoch120:Loss=8.2156\n",
      "Epoch140:Loss=8.1528\n",
      "Epoch160:Loss=6.0935\n",
      "Epoch180:Loss=5.3221\n",
      "Epoch0:Loss=21.5369\n",
      "Epoch20:Loss=15.7259\n",
      "Epoch40:Loss=11.6495\n",
      "Epoch60:Loss=8.4069\n",
      "Epoch80:Loss=6.6604\n",
      "Epoch100:Loss=6.7919\n",
      "Epoch120:Loss=5.6023\n",
      "Epoch140:Loss=5.1742\n",
      "Epoch160:Loss=5.0141\n",
      "Epoch180:Loss=4.9605\n",
      "Epoch0:Loss=23.2782\n",
      "Epoch20:Loss=16.2461\n",
      "Epoch40:Loss=13.0019\n",
      "Epoch60:Loss=9.3628\n",
      "Epoch80:Loss=10.6183\n",
      "Epoch100:Loss=7.8569\n",
      "Epoch120:Loss=7.3724\n",
      "Epoch140:Loss=7.1643\n",
      "Epoch160:Loss=9.2632\n",
      "Epoch180:Loss=8.5432\n",
      "Epoch0:Loss=25.0247\n",
      "Epoch20:Loss=24.6351\n",
      "Epoch40:Loss=11.0660\n",
      "Epoch60:Loss=10.5258\n",
      "Epoch80:Loss=9.0480\n",
      "Epoch100:Loss=10.8471\n",
      "Epoch120:Loss=8.7411\n",
      "Epoch140:Loss=8.4821\n",
      "Epoch160:Loss=8.4927\n",
      "Epoch180:Loss=8.3876\n",
      "Epoch0:Loss=21.7522\n",
      "Epoch20:Loss=12.2744\n",
      "Epoch40:Loss=10.5419\n",
      "Epoch60:Loss=6.0805\n",
      "Epoch80:Loss=6.3445\n",
      "Epoch100:Loss=8.3377\n",
      "Epoch120:Loss=5.9664\n",
      "Epoch140:Loss=5.7641\n",
      "Epoch160:Loss=7.8902\n",
      "Epoch180:Loss=5.6221\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 8\u001b[0m     loss ,embedding,attention \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43mS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#print(embedding)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32me:\\ana\\envs\\wuyu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\ana\\envs\\wuyu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[116], line 21\u001b[0m, in \u001b[0;36mGATE.forward\u001b[1;34m(self, A, X, R, S)\u001b[0m\n\u001b[0;32m     19\u001b[0m H \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[1;32m---> 21\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#最终节点表征\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH \u001b[38;5;241m=\u001b[39m H\n",
      "Cell \u001b[1;32mIn[116], line 47\u001b[0m, in \u001b[0;36mGATE.encoder\u001b[1;34m(self, A, H, layer)\u001b[0m\n\u001b[0;32m     45\u001b[0m H \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(H,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW[layer]\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt())\n\u001b[0;32m     46\u001b[0m H \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBatchNorm1d(H\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))(H) \n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC[layer]\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_attention_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m H \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC[layer],H)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m H\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m identity\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "Cell \u001b[1;32mIn[116], line 77\u001b[0m, in \u001b[0;36mGATE.graph_attention_layer\u001b[1;34m(self, A, M, v, layer, threshold)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m#启用异常检测，监控反向传播\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m#计算全连接注意力分数\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     f2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(M,v[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt())\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m#生成全连接分数矩阵N,N\u001b[39;00m\n",
      "File \u001b[1;32me:\\ana\\envs\\wuyu\\lib\\site-packages\\torch\\fx\\traceback.py:67\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32me:\\ana\\envs\\wuyu\\lib\\traceback.py:211\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 211\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32me:\\ana\\envs\\wuyu\\lib\\traceback.py:362\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    359\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    360\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals))\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m--> 362\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32me:\\ana\\envs\\wuyu\\lib\\linecache.py:74\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 74\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(num_cities):\n",
    "    x = torch.from_numpy(data_3d[i]).float()\n",
    "    model = GATE(hidden_dims,A,x,S,R)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.0002)\n",
    "    for epoch in range(200):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss ,embedding,attention = model(A,x,R,S)\n",
    "        #print(embedding)\n",
    "        loss.backward()\n",
    "        # 在训练循环中添加梯度检查\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.grad is None:\n",
    "        #         print(f\"参数 {name} 无梯度\")\n",
    "        #     else:\n",
    "        #         print(f\"参数 {param.shape} 梯度范数: {param.grad.norm().item():.4f},是否可训练{param.requires_grad}\")\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch{epoch}:Loss={loss.item():.4f}\")\n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATE(nn.Module):\n",
    "    def __init__(self,hidden_dims,A,X,S,R,lambda_=0.05):\n",
    "        super().__init__()\n",
    "        self.lambda_ = lambda_\n",
    "        self.n_layers = len(hidden_dims)-1\n",
    "        self.W,self.v = self.define_weights(hidden_dims)\n",
    "        self.C={}\n",
    "        self.threshold = 0.01\n",
    "        self.A = A\n",
    "        self.X = X\n",
    "        self.S = S\n",
    "        self.R = R\n",
    "        self.res_linear = nn.ModuleList([nn.Linear(hidden_dims[i],hidden_dims[i+1])for i in range(len(hidden_dims)-1)])\n",
    "        self.apply(self._init_weights)\n",
    "    #前向传播\n",
    "    def forward(self,A,X,R,S):\n",
    "\n",
    "        #编码\n",
    "        H = X\n",
    "        for layer in range(self.n_layers):\n",
    "            H = self.encoder(A, H, layer)\n",
    "        #最终节点表征\n",
    "        self.H = H\n",
    "\n",
    "        #解码\n",
    "        for layer in range(self.n_layers-1,-1,-1):\n",
    "            H = self.decoder(H,layer)\n",
    "        X_ = H\n",
    "\n",
    "        #节点得重构损失\n",
    "        features_loss = torch.sqrt(torch.sum((X-X_)**2))\n",
    "\n",
    "        #图结构得重构损失\n",
    "        S_emb = self.H[S]\n",
    "        R_emb = self.H[R]\n",
    "        structure_loss = -torch.log(torch.sigmoid(torch.sum(S_emb*R_emb,dim=-1))).sum()\n",
    "\n",
    "        #总损失\n",
    "        self.loss = features_loss + self.lambda_*structure_loss\n",
    "\n",
    "        return self.loss,self.H,self.C\n",
    "    def encoder(self,A,H,layer):\n",
    "        identity = H\n",
    "        H = nn.LeakyReLU(0.2)(H)\n",
    "        H = torch.matmul(H,self.W[layer].weight.t())\n",
    "        H = nn.BatchNorm1d(H.size(1))(H) \n",
    "        self.C[layer]= self.graph_attention_layer(A,H,self.v[layer],layer,self.threshold)\n",
    "        H = torch.matmul(self.C[layer],H)\n",
    "        if H.shape[-1] == identity.shape[-1]:\n",
    "            H = H+identity\n",
    "        else:\n",
    "            H = H+self.res_linear[layer](identity)\n",
    "        return H \n",
    "    #解码器\n",
    "    def decoder(self,H,layer):\n",
    "        H = torch.matmul(H,self.W[layer].weight)\n",
    "        H = nn.BatchNorm1d(H.size(1))(H)   #添加 BatchNorm\n",
    "        H = nn.ReLU()(H)\n",
    "        return torch.matmul(self.C[layer],H)\n",
    "    \n",
    "    def define_weights(self,hidden_dims):\n",
    "        W = nn.ModuleList(nn.Linear(hidden_dims[i],hidden_dims[i+1])for i in range(self.n_layers))\n",
    "        Ws_att = nn.ModuleList([nn.ModuleList([nn.Linear(hidden_dims[i+1],1,bias=False),nn.Linear(hidden_dims[i+1],1,bias=False)])for i in range(self.n_layers)])\n",
    "        return W,Ws_att\n",
    "    def graph_attention_layer(self,A,M,v,layer,threshold):\n",
    "        '''\n",
    "        全连接图注意力层（带阈值剪枝）\n",
    "        参数：\n",
    "        M:节点特征矩阵N,d\n",
    "        v:注意力参数列表,包含两个可学习向量v1,v2每个形状d,1\n",
    "        layer:层编号\n",
    "        '''\n",
    "        #启用异常检测，监控反向传播\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            #计算全连接注意力分数\n",
    "            f1 = torch.matmul(M,v[0].weight.t())\n",
    "            f2 = torch.matmul(M,v[1].weight.t())\n",
    "\n",
    "            #生成全连接分数矩阵N,N\n",
    "            logits = f1 + f2.T\n",
    "            logits = F.dropout(logits,p=0.1,training=self.training)\n",
    "            logits = logits / torch.max(torch.abs(logits))   #归一化\n",
    "            #激活函数\n",
    "            unnormalized_attentions = torch.exp(logits)\n",
    "            \n",
    "        #     #行方向归一化\n",
    "            attentions = F.softmax(unnormalized_attentions,dim=1)\n",
    "         \n",
    "            #print(attentions.mean(), attentions.min(), attentions.max())\n",
    "        #     #阈值剪枝\n",
    "            return attentions\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.ModuleList):   #针对注意力参数的特殊初始化\n",
    "            for sub_module in m:\n",
    "                if isinstance(sub_module, nn.Linear):\n",
    "                    nn.init.normal_(sub_module.weight, mean=0, std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([292, 22, 5])\n",
      "(292, 110)\n"
     ]
    }
   ],
   "source": [
    "data = torch.stack(embeddings)\n",
    "print(data.shape)\n",
    "numpy_data = data.detach().numpy()\n",
    "numpy_data = numpy_data.reshape(292,110)\n",
    "print(numpy_data.shape)\n",
    "df = pd.DataFrame(numpy_data)\n",
    "df.to_excel('embedding.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_s = torch.tensor(numpy_data,dtype=torch.float32)\n",
    "model = GATEWithClustering(embedding_s)\n",
    "cluster_labels = model.cluster_cities()\n",
    "print(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATEWithClustering():\n",
    "    def __init__(self,embeddings):\n",
    "        super().__init__()\n",
    "        self.cluster_labels = None\n",
    "        self.cluster_centers = None\n",
    "        self.lambda_ = 0.01\n",
    "        self.embeddings = embeddings\n",
    "    def cluster_cities(self,n_clusters = 3,visualize = True):\n",
    "        '''\n",
    "        城市韧性等级聚类(5类)\n",
    "        参数：\n",
    "          n_cluters:聚类数量（韧性等级数）\n",
    "          visualize:是否进行降维可视化\n",
    "        '''\n",
    "        #获取节点嵌入\n",
    "        X_emb =self.embeddings\n",
    "\n",
    "        #使用KMeans聚类\n",
    "        kmeans = KMeans(n_clusters=n_clusters,random_state = 42,n_init=10)\n",
    "        self.cluster_labels = kmeans.fit_predict(X_emb)\n",
    "        self.cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "        #聚类结果分析\n",
    "        self._analyze_clusters()\n",
    "\n",
    "        #可视化嵌入空间\n",
    "        if visualize and X_emb.shape[1]>2:\n",
    "            self._visualize_embeddings(X_emb)\n",
    "\n",
    "        return self.cluster_labels\n",
    "    \n",
    "    def _analyze_clusters(self):\n",
    "        \"\"\"分析聚类结果\"\"\"\n",
    "        unique,counts = np.unique(self.cluster_labels,return_counts=True)\n",
    "        print(\"\\n城市韧性等级分布\")\n",
    "        for cls,cnt in zip(unique,counts):\n",
    "            print(f\"等级{cls+1}:{cnt}个城市(占比{cnt/len(self.cluster_labels):.1%})\")\n",
    "        #计算轮廓系数\n",
    "        from sklearn.metrics import silhouette_score\n",
    "        sil_score = silhouette_score(self.embeddings.detach().cpu().numpy(),self.cluster_labels)\n",
    "        print(f\"\\n轮廓系数:{sil_score:.3f}(越接近1表示聚类效果越好)\")\n",
    "\n",
    "    def _visualize_embeddings(self,X_emb,perplexity=2):\n",
    "        \"\"\"使用t-SNE降维可视化\"\"\"\n",
    "        tsne = TSNE(n_components=2,perplexity=perplexity,random_state=42)\n",
    "        X_tsne = tsne.fit_transform(X_emb)\n",
    "\n",
    "        plt.figure(figsize=(10,8))\n",
    "        scatter = plt.scatter(X_tsne[:,0],X_tsne[:,1],\n",
    "                              c = self.cluster_labels,\n",
    "                              cmap='viridis',\n",
    "                              alpha=0.7,\n",
    "                              edgecolors='w')\n",
    "        #添加图例和标签\n",
    "        plt.colorbar(scatter,ticks=range(5),label='韧性等级')\n",
    "        plt.title(\"城市韧性嵌入空间可视化(t-SNE)\")\n",
    "        plt.xlabel(\"t-SNE 1\")\n",
    "        plt.ylabel(\"t-SNE 2\")\n",
    "\n",
    "        #标记聚类中心\n",
    "        centers_tsne = tsne.fit_transform(self.cluster_centers)\n",
    "        plt.scatter(centers_tsne[:,0],centers_tsne[:,1],\n",
    "                    c='red',marker='X',s=200,\n",
    "                    edgecolors='k',linewidths=1.5,\n",
    "                    label='等级中心')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wuyu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
